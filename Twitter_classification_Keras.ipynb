{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f68833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c17a7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c189bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked',\n",
       "       'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze',\n",
       "       'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood',\n",
       "       'bloody', 'blown%20up', 'body%20bag', 'body%20bagging',\n",
       "       'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse',\n",
       "       'buildings%20burning', 'buildings%20on%20fire', 'burned',\n",
       "       'burning', 'burning%20buildings', 'bush%20fires', 'casualties',\n",
       "       'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency',\n",
       "       'cliff%20fall', 'collapse', 'collapsed', 'collide', 'collided',\n",
       "       'collision', 'crash', 'crashed', 'crush', 'crushed', 'curfew',\n",
       "       'cyclone', 'damage', 'danger', 'dead', 'death', 'deaths', 'debris',\n",
       "       'deluge', 'deluged', 'demolish', 'demolished', 'demolition',\n",
       "       'derail', 'derailed', 'derailment', 'desolate', 'desolation',\n",
       "       'destroy', 'destroyed', 'destruction', 'detonate', 'detonation',\n",
       "       'devastated', 'devastation', 'disaster', 'displaced', 'drought',\n",
       "       'drown', 'drowned', 'drowning', 'dust%20storm', 'earthquake',\n",
       "       'electrocute', 'electrocuted', 'emergency', 'emergency%20plan',\n",
       "       'emergency%20services', 'engulfed', 'epicentre', 'evacuate',\n",
       "       'evacuated', 'evacuation', 'explode', 'exploded', 'explosion',\n",
       "       'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear',\n",
       "       'fire', 'fire%20truck', 'first%20responders', 'flames',\n",
       "       'flattened', 'flood', 'flooding', 'floods', 'forest%20fire',\n",
       "       'forest%20fires', 'hail', 'hailstorm', 'harm', 'hazard',\n",
       "       'hazardous', 'heat%20wave', 'hellfire', 'hijack', 'hijacker',\n",
       "       'hijacking', 'hostage', 'hostages', 'hurricane', 'injured',\n",
       "       'injuries', 'injury', 'inundated', 'inundation', 'landslide',\n",
       "       'lava', 'lightning', 'loud%20bang', 'mass%20murder',\n",
       "       'mass%20murderer', 'massacre', 'mayhem', 'meltdown', 'military',\n",
       "       'mudslide', 'natural%20disaster', 'nuclear%20disaster',\n",
       "       'nuclear%20reactor', 'obliterate', 'obliterated', 'obliteration',\n",
       "       'oil%20spill', 'outbreak', 'pandemonium', 'panic', 'panicking',\n",
       "       'police', 'quarantine', 'quarantined', 'radiation%20emergency',\n",
       "       'rainstorm', 'razed', 'refugees', 'rescue', 'rescued', 'rescuers',\n",
       "       'riot', 'rioting', 'rubble', 'ruin', 'sandstorm', 'screamed',\n",
       "       'screaming', 'screams', 'seismic', 'sinkhole', 'sinking', 'siren',\n",
       "       'sirens', 'smoke', 'snowstorm', 'storm', 'stretcher',\n",
       "       'structural%20failure', 'suicide%20bomb', 'suicide%20bomber',\n",
       "       'suicide%20bombing', 'sunk', 'survive', 'survived', 'survivors',\n",
       "       'terrorism', 'terrorist', 'threat', 'thunder', 'thunderstorm',\n",
       "       'tornado', 'tragedy', 'trapped', 'trauma', 'traumatised',\n",
       "       'trouble', 'tsunami', 'twister', 'typhoon', 'upheaval',\n",
       "       'violent%20storm', 'volcano', 'war%20zone', 'weapon', 'weapons',\n",
       "       'whirlwind', 'wild%20fires', 'wildfire', 'windstorm', 'wounded',\n",
       "       'wounds', 'wreck', 'wreckage', 'wrecked'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keyword'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb2c4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "body%20bags              41\n",
       "harm                     41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee10928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Birmingham', 'Est. September 2012 - Bristol', ...,\n",
       "       'Vancouver, Canada', 'London ', 'Lincoln'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cffbbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                      104\n",
       "New York                  71\n",
       "United States             50\n",
       "London                    45\n",
       "Canada                    29\n",
       "                        ... \n",
       "Manavadar, Gujarat         1\n",
       "Leicester, England         1\n",
       "Augusta, Maine, 04330      1\n",
       "PARACHUTE                  1\n",
       "taco bell                  1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "595f5acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                               4\n",
       "keyword                                        NaN\n",
       "location                                       NaN\n",
       "text        Forest fire near La Ronge Sask. Canada\n",
       "target                                           1\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6abc00b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-557d5165a0fe>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'][count] = j.text\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "df['label'] = df['location']\n",
    "\n",
    "for i in df['text']:\n",
    "    \n",
    "    doc = nlp(i)\n",
    "    for j in doc.ents:\n",
    "        if (j.label_ == 'GPE') and (pd.isnull(df['location'][count])): \n",
    "            #print(df.loc[count])\n",
    "            #print(\"Text:\", j.text)\n",
    "            #print(\"Label\", j.label_)\n",
    "            #print(df['location'][count])\n",
    "            \n",
    "            df['label'][count] = j.text\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0f09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_location(df_x):\n",
    "    \n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    doc = nlp(df_x['text'])\n",
    "    try:\n",
    "        if doc.ents[0].label_ == 'GPE':\n",
    "            #print(\" printing label text\", doc.ents[0].text)\n",
    "            df_x['label'][count]==doc.ents[0].text\n",
    "            count+=1\n",
    "    except:\n",
    "            count+=1\n",
    "   \n",
    "    return df_x['label']\n",
    "    \n",
    "#df['label'] = df['location']\n",
    "#\n",
    "#df['label'] = df.apply(lambda x: fill_location(x), axis=1)\n",
    "\n",
    "\n",
    "#print(df_new['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0f261d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3381f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                      105\n",
       "New York                  72\n",
       "United States             50\n",
       "London                    48\n",
       "California                32\n",
       "                        ... \n",
       "Leicester, England         1\n",
       "Augusta, Maine, 04330      1\n",
       "PARACHUTE                  1\n",
       "Alberta, VA                1\n",
       "taco bell                  1\n",
       "Name: label, Length: 3517, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7016c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'‘s', 'very', 'at', 'twenty', 'of', 'themselves', 'beyond', 'often', 'put', 'get', 'somewhere', 'our', 'other', 'please', 'yourself', 'is', 'becoming', 'whether', 'whereafter', 'they', 'only', 'somehow', 'among', 'throughout', 'anything', 'sometimes', 'been', 'their', 'none', 'afterwards', 'yourselves', 'side', '’d', 'latterly', 'rather', 'could', 'someone', 'since', 'back', 'regarding', 'just', 'this', 'that', 'unless', 'do', 'no', 'which', 'until', 'done', 'move', 'much', '‘d', 'hers', 'never', 'nine', 'thereupon', 'under', 'former', 'elsewhere', 'next', 'can', 'front', 'herself', 'us', 'what', 'hereupon', 'a', 'bottom', 'perhaps', 'nor', 'above', 'enough', 'upon', 'whole', 'take', 'see', 'thereby', 'name', 'too', 'nobody', 'nevertheless', \"'d\", 'meanwhile', 'neither', 'without', 'be', '‘m', 'up', 'herein', 'own', 'wherein', 'toward', 'well', 'wherever', 'still', 'quite', 'together', 'amongst', 'him', 'where', \"'ve\", '’ve', 'mostly', 'by', 'are', 'as', 'cannot', 're', 'go', 'if', 'few', 'being', 'same', 'doing', 'yours', 'them', 'through', 'would', 'seem', 'against', \"'m\", '‘ve', 'most', 'we', 'while', 'almost', 'ours', 'sixty', 'behind', 'hereby', 'ourselves', 'two', 'anyhow', 'were', 'might', 'one', 'so', 'part', 'hereafter', 'have', 'then', 'whence', 'off', 'otherwise', 'over', 'whither', 'via', \"'re\", '’s', 'you', 'than', 'several', 'for', 'any', 'latter', '‘re', 'around', 'therein', 'from', 'four', '’m', 'who', 'both', 'not', 'once', 'nowhere', 'anyone', 'keep', 'hundred', 'seeming', 'myself', 'third', 'except', 'six', 'indeed', 'however', 'anywhere', 'made', 'between', 'it', 'must', 'therefore', 'down', 'three', 'everyone', 'another', 'show', 'least', 'ten', 'in', 'itself', 'else', 'fifty', 'whatever', 'really', 'to', 'he', 'these', 'something', 'serious', 'everywhere', 'an', 'besides', 'towards', 'within', \"n't\", 'first', 'more', 'less', 'beside', 'the', 'n‘t', 'before', 'even', 'about', 'alone', 'had', 'himself', 'ca', 'everything', 'five', 'further', 'whereupon', 'here', 'out', 'others', 'give', 'n’t', 'say', 'each', 'beforehand', 'along', \"'s\", 'may', 'full', 'last', 'always', '‘ll', 'i', 'call', 'become', 'again', 'will', '’re', 'thru', 'across', 'such', 'now', 'top', 'whereas', 'sometime', 'all', 'has', 'did', 'though', 'your', 'twelve', 'forty', \"'ll\", 'noone', 'how', 'thereafter', 'those', 'and', 'either', 'seemed', 'amount', 'formerly', 'also', 'or', 'every', 'me', 'its', 'eight', 'whenever', 'hence', 'but', 'mine', 'with', 'his', 'moreover', 'anyway', 'becomes', 'thus', 'why', 'after', 'many', 'using', 'eleven', 'when', 'nothing', 'should', 'thence', 'her', 'was', 'onto', 'make', 'ever', '’ll', 'am', 'various', 'namely', 'already', 'due', 'yet', 'below', 'some', 'although', 'because', 'does', 'fifteen', 'my', 'per', 'there', 'whoever', 'whom', 'during', 'into', 'seems', 'became', 'whose', 'empty', 'used', 'whereby', 'she', 'on'}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882aafc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c280988d88ea>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_no_stop'][count] = re.sub(r'[^0-9a-zA-Z_//.]+',\" \",str(words))\n"
     ]
    }
   ],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "df['text_no_stop'] = df['text']\n",
    "\n",
    "for text in df['text']:\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        if word.lower() not in stop_words:\n",
    "            words.append(word)\n",
    "            \n",
    "    df['text_no_stop'][count] = re.sub(r'[^0-9a-zA-Z_//.]+',\" \",str(words))\n",
    "    count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a63e1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = round(len(df)*.665)\n",
    "train_data = df.iloc[:train_data_size]\n",
    "val_data = df.iloc[train_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df36fbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "      <th>text_no_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deeds Reason earthquake ALLAH Forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>California</td>\n",
       "      <td>13 000 people receive wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo Ruby Alaska smoke wildfires po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>giant cranes holding bridge collapse nearby h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>California</td>\n",
       "      <td>aria_ahrary TheTawniest control wild fires Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 01 04 UTC 5km S Volcano Hawaii. http //...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating e bike collided car Litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Latest Homes Razed Northern California Wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target       label  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1         NaN   \n",
       "1                Forest fire near La Ronge Sask. Canada       1      Canada   \n",
       "2     All residents asked to 'shelter in place' are ...       1         NaN   \n",
       "3     13,000 people receive #wildfires evacuation or...       1  California   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1         NaN   \n",
       "...                                                 ...     ...         ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1         NaN   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  California   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1         NaN   \n",
       "7611  Police investigating after an e-bike collided ...       1         NaN   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1         NaN   \n",
       "\n",
       "                                           text_no_stop  \n",
       "0                Deeds Reason earthquake ALLAH Forgive   \n",
       "1               Forest fire near La Ronge Sask. Canada   \n",
       "2      residents asked shelter place notified office...  \n",
       "3      13 000 people receive wildfires evacuation or...  \n",
       "4      got sent photo Ruby Alaska smoke wildfires po...  \n",
       "...                                                 ...  \n",
       "7608   giant cranes holding bridge collapse nearby h...  \n",
       "7609   aria_ahrary TheTawniest control wild fires Ca...  \n",
       "7610   M1.94 01 04 UTC 5km S Volcano Hawaii. http //...  \n",
       "7611   Police investigating e bike collided car Litt...  \n",
       "7612   Latest Homes Razed Northern California Wildfi...  \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd04d4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13,000 people receive #wildfires evacuation orders in California \n",
      "13,000 people receive wildfires evacuation orders in California \n"
     ]
    }
   ],
   "source": [
    "x = df['text'][3]\n",
    "print(x)\n",
    "\n",
    "y = re.sub(r'[^(,\\d+)0-9,a-zA-Z_//.]+',\" \",str(x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9ff2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3be0960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "        lowercase = tf.strings.lower(input_data)\n",
    "        stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "        return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d24e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.convert_to_tensor(train_data['text'])\n",
    "target = tf.convert_to_tensor(train_data['target'])\n",
    "input_val = tf.convert_to_tensor(val_data['text'])\n",
    "target_val = tf.convert_to_tensor(val_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80a3e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'\n",
      " b'Forest fire near La Ronge Sask. Canada'\n",
      " b\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
      " ...\n",
      " b'Rise up like a natural disaster we take the bat and then we take back the town????'\n",
      " b'its only getting colder and colder and faster and faster and when i first realized it it was like a natural disaster'\n",
      " b'anyway 2 me? Mateo just doesnt exist? Hes a mirage a pointless addition to our Generation. a human natural disaster. Im sorry but its true'], shape=(5063,), dtype=string)\n",
      "tf.Tensor([1 1 1 ... 0 1 0], shape=(5063,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[b'natural disaster \\xc2\\x89\\xc3\\x9b\\xc3\\x92 News Stories About natural disaster - Page 1 | Newser http://t.co/TB8gZEMbXU'\n",
      " b\"And you're loving me like water slipping through my fingers such a natural disaster love\"\n",
      " b'I added a video to a @YouTube playlist http://t.co/v2yXurne2p Natural Disaster Survival - HUG BY A GUEST!! on Roblox'\n",
      " ... b'M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ'\n",
      " b'Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.'\n",
      " b'The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d'], shape=(2550,), dtype=string)\n",
      "tf.Tensor([1 0 0 ... 1 1 1], shape=(2550,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(input)\n",
    "print(target)\n",
    "print(input_val)\n",
    "print(target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8dcd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((input,target))\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((input_val,target_val))\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9693007",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    #standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "vectorize_layer.adapt(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5e3c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcb162eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'\n",
      " b'Forest fire near La Ronge Sask. Canada'\n",
      " b\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
      " ... b'M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ'\n",
      " b'Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.'\n",
      " b'The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d']\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    input = tf.convert_to_tensor(df['text'])\n",
    "    output = tf.convert_to_tensor(df['target'])\n",
    "    print(input.eval())\n",
    "    print(output.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e935d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8539d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a041dabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/1266 [..............................] - ETA: 1:01 - loss: 0.6829 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0917s). Check your callbacks.\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.6829 - accuracy: 0.5819 - val_loss: 0.6816 - val_accuracy: 0.5475\n",
      "Epoch 2/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.6506 - accuracy: 0.5831 - val_loss: 0.6371 - val_accuracy: 0.5475\n",
      "Epoch 3/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.5328 - accuracy: 0.7061 - val_loss: 0.5610 - val_accuracy: 0.6475\n",
      "Epoch 4/15\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 0.4087 - accuracy: 0.8151 - val_loss: 0.5170 - val_accuracy: 0.7282\n",
      "Epoch 5/15\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 0.3298 - accuracy: 0.8552 - val_loss: 0.5015 - val_accuracy: 0.7498\n",
      "Epoch 6/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.2711 - accuracy: 0.8845 - val_loss: 0.5066 - val_accuracy: 0.7584\n",
      "Epoch 7/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.2254 - accuracy: 0.9056 - val_loss: 0.5262 - val_accuracy: 0.7620\n",
      "Epoch 8/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.1892 - accuracy: 0.9212 - val_loss: 0.5527 - val_accuracy: 0.7592\n",
      "Epoch 9/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.1593 - accuracy: 0.9344 - val_loss: 0.5870 - val_accuracy: 0.7635\n",
      "Epoch 10/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.1340 - accuracy: 0.9463 - val_loss: 0.6323 - val_accuracy: 0.7561\n",
      "Epoch 11/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.1130 - accuracy: 0.9550 - val_loss: 0.6923 - val_accuracy: 0.7435\n",
      "Epoch 12/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.0961 - accuracy: 0.9595 - val_loss: 0.7641 - val_accuracy: 0.7353\n",
      "Epoch 13/15\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 0.0830 - accuracy: 0.9670 - val_loss: 0.8391 - val_accuracy: 0.7294\n",
      "Epoch 14/15\n",
      "1266/1266 [==============================] - 3s 3ms/step - loss: 0.0733 - accuracy: 0.9706 - val_loss: 0.9067 - val_accuracy: 0.7184\n",
      "Epoch 15/15\n",
      "1266/1266 [==============================] - 4s 3ms/step - loss: 0.0659 - accuracy: 0.9741 - val_loss: 0.9622 - val_accuracy: 0.7137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20dfde805e0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6710a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id keyword location  \\\n",
      "0         0     NaN      NaN   \n",
      "1         2     NaN      NaN   \n",
      "2         3     NaN      NaN   \n",
      "3         9     NaN      NaN   \n",
      "4        11     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "3258  10861     NaN      NaN   \n",
      "3259  10865     NaN      NaN   \n",
      "3260  10868     NaN      NaN   \n",
      "3261  10874     NaN      NaN   \n",
      "3262  10875     NaN      NaN   \n",
      "\n",
      "                                                   text  \n",
      "0                    Just happened a terrible car crash  \n",
      "1     Heard about #earthquake is different cities, s...  \n",
      "2     there is a forest fire at spot pond, geese are...  \n",
      "3              Apocalypse lighting. #Spokane #wildfires  \n",
      "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
      "...                                                 ...  \n",
      "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
      "3259  Storm in RI worse than last hurricane. My city...  \n",
      "3260  Green Line derailment in Chicago http://t.co/U...  \n",
      "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
      "3262  #CityofCalgary has activated its Municipal Eme...  \n",
      "\n",
      "[3263 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d49bd42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11023994]\n",
      " [3.9487932 ]\n",
      " [7.0969977 ]\n",
      " ...\n",
      " [6.832763  ]\n",
      " [3.345881  ]\n",
      " [6.595546  ]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "\n",
    "input_test = tf.convert_to_tensor(df_test['text'])\n",
    "\n",
    "predict = model.predict(input_test)\n",
    "\n",
    "print(predict )\n",
    "\n",
    "#predict = scaler.inverse_transform(scaler_predict)\n",
    "\n",
    "#print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b6ae4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.11023994]\n",
      " [3.9487932 ]\n",
      " [7.0969977 ]\n",
      " ...\n",
      " [6.832763  ]\n",
      " [3.345881  ]\n",
      " [6.595546  ]], shape=(3263, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.nn.relu(predict, name = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "94eed399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11023994]\n"
     ]
    }
   ],
   "source": [
    "print(np.maximum(predict[0],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9a2814a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "3258    1\n",
      "3259    1\n",
      "3260    1\n",
      "3261    1\n",
      "3262    1\n",
      "Length: 3263, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def ReLU(x):\n",
    "\n",
    "    if np.maximum(0,x) == 0.00:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    #return (np.maximum(0,x))\n",
    "\n",
    "predict_relu = []\n",
    "\n",
    "for i in predict:\n",
    "    predict_relu.append(ReLU(i))\n",
    "    \n",
    "\n",
    "test_target = pd.Series(predict_relu)\n",
    "\n",
    "print(test_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "efc8c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      Just happened a terrible car crash\n",
      "1       Heard about #earthquake is different cities, s...\n",
      "2       there is a forest fire at spot pond, geese are...\n",
      "3                Apocalypse lighting. #Spokane #wildfires\n",
      "4           Typhoon Soudelor kills 28 in China and Taiwan\n",
      "                              ...                        \n",
      "3258    EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
      "3259    Storm in RI worse than last hurricane. My city...\n",
      "3260    Green Line derailment in Chicago http://t.co/U...\n",
      "3261    MEG issues Hazardous Weather Outlook (HWO) htt...\n",
      "3262    #CityofCalgary has activated its Municipal Eme...\n",
      "Name: text, Length: 3263, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8fec4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['target'] = test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e189ccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id keyword location  \\\n",
      "0         0     NaN      NaN   \n",
      "1         2     NaN      NaN   \n",
      "2         3     NaN      NaN   \n",
      "3         9     NaN      NaN   \n",
      "4        11     NaN      NaN   \n",
      "...     ...     ...      ...   \n",
      "3258  10861     NaN      NaN   \n",
      "3259  10865     NaN      NaN   \n",
      "3260  10868     NaN      NaN   \n",
      "3261  10874     NaN      NaN   \n",
      "3262  10875     NaN      NaN   \n",
      "\n",
      "                                                   text  target  \n",
      "0                    Just happened a terrible car crash       1  \n",
      "1     Heard about #earthquake is different cities, s...       1  \n",
      "2     there is a forest fire at spot pond, geese are...       1  \n",
      "3              Apocalypse lighting. #Spokane #wildfires       1  \n",
      "4         Typhoon Soudelor kills 28 in China and Taiwan       1  \n",
      "...                                                 ...     ...  \n",
      "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...       1  \n",
      "3259  Storm in RI worse than last hurricane. My city...       1  \n",
      "3260  Green Line derailment in Chicago http://t.co/U...       1  \n",
      "3261  MEG issues Hazardous Weather Outlook (HWO) htt...       1  \n",
      "3262  #CityofCalgary has activated its Municipal Eme...       1  \n",
      "\n",
      "[3263 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ebc74348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['id','target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d28012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
